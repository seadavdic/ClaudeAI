apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-demo-code
  namespace: kafka-demo
data:
  requirements.txt: |
    kafka-python==2.0.2
    prometheus_client==0.19.0

  producer.py: |
    #!/usr/bin/env python3
    """
    Kafka Producer Demo - Publishes SmartBiz events
    """
    from kafka import KafkaProducer
    import json
    import time
    import random
    from datetime import datetime
    from prometheus_client import Counter, Histogram, start_http_server
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Prometheus metrics
    messages_sent = Counter('kafka_messages_sent_total', 'Total messages sent', ['topic'])
    send_duration = Histogram('kafka_send_duration_seconds', 'Time to send message', ['topic'])

    # Kafka broker
    KAFKA_BROKER = 'kafka.kafka.svc.cluster.local:9092'

    # Sample data
    PRODUCTS = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']
    CUSTOMERS = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']
    EVENTS = ['order_created', 'stock_updated', 'customer_registered', 'payment_processed']

    def create_producer():
        """Create Kafka producer with JSON serializer"""
        return KafkaProducer(
            bootstrap_servers=[KAFKA_BROKER],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None,
            acks='all',  # Wait for all replicas to acknowledge
            retries=3,
            max_in_flight_requests_per_connection=1  # Guarantee ordering
        )

    def generate_event():
        """Generate random SmartBiz event"""
        event_type = random.choice(EVENTS)

        if event_type == 'order_created':
            return {
                'event_type': 'order_created',
                'order_id': random.randint(1000, 9999),
                'customer': random.choice(CUSTOMERS),
                'product': random.choice(PRODUCTS),
                'quantity': random.randint(1, 5),
                'price': round(random.uniform(10, 1000), 2),
                'timestamp': datetime.utcnow().isoformat()
            }
        elif event_type == 'stock_updated':
            return {
                'event_type': 'stock_updated',
                'product': random.choice(PRODUCTS),
                'old_stock': random.randint(0, 100),
                'new_stock': random.randint(0, 100),
                'timestamp': datetime.utcnow().isoformat()
            }
        elif event_type == 'customer_registered':
            return {
                'event_type': 'customer_registered',
                'customer': random.choice(CUSTOMERS),
                'email': f"{random.choice(CUSTOMERS).lower()}@example.com",
                'timestamp': datetime.utcnow().isoformat()
            }
        else:  # payment_processed
            return {
                'event_type': 'payment_processed',
                'order_id': random.randint(1000, 9999),
                'amount': round(random.uniform(10, 1000), 2),
                'status': random.choice(['success', 'failed']),
                'timestamp': datetime.utcnow().isoformat()
            }

    def main():
        # Start Prometheus metrics server
        start_http_server(8000)
        logger.info("Prometheus metrics available at :8000/metrics")

        # Create producer
        producer = create_producer()
        logger.info(f"Connected to Kafka broker: {KAFKA_BROKER}")

        # Publish events continuously
        event_count = 0
        while True:
            try:
                # Generate event
                event = generate_event()
                topic = f"smartbiz-{event['event_type'].replace('_', '-')}"
                key = event.get('order_id') or event.get('customer')

                # Send to Kafka with timing
                start_time = time.time()
                future = producer.send(topic, key=str(key), value=event)
                future.get(timeout=10)  # Wait for confirmation
                duration = time.time() - start_time

                # Update metrics
                messages_sent.labels(topic=topic).inc()
                send_duration.labels(topic=topic).observe(duration)

                event_count += 1
                logger.info(f"Sent {event_count}: {event['event_type']} to topic '{topic}' (key={key})")

                # Send events every 5 seconds
                time.sleep(5)

            except Exception as e:
                logger.error(f"Error sending message: {e}")
                time.sleep(10)

    if __name__ == '__main__':
        main()

  consumer.py: |
    #!/usr/bin/env python3
    """
    Kafka Consumer Demo - Shows different consumption patterns
    """
    from kafka import KafkaConsumer
    import json
    import os
    from prometheus_client import Counter, Gauge, start_http_server
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Prometheus metrics
    messages_received = Counter('kafka_messages_received_total', 'Total messages received', ['topic', 'consumer_group'])
    lag = Gauge('kafka_consumer_lag', 'Consumer lag', ['topic', 'consumer_group'])

    # Configuration from environment
    KAFKA_BROKER = 'kafka.kafka.svc.cluster.local:9092'
    CONSUMER_GROUP = os.getenv('CONSUMER_GROUP', 'default-group')
    TOPICS = os.getenv('TOPICS', 'smartbiz-order-created').split(',')
    AUTO_OFFSET_RESET = os.getenv('AUTO_OFFSET_RESET', 'latest')  # or 'earliest' for replay

    def create_consumer():
        """Create Kafka consumer"""
        return KafkaConsumer(
            *TOPICS,
            bootstrap_servers=[KAFKA_BROKER],
            group_id=CONSUMER_GROUP,
            auto_offset_reset=AUTO_OFFSET_RESET,
            enable_auto_commit=True,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )

    def process_message(message):
        """Process received message"""
        event = message.value
        event_type = event.get('event_type', 'unknown')

        logger.info(f"[{CONSUMER_GROUP}] Received {event_type} from topic '{message.topic}' "
                   f"partition {message.partition}, offset {message.offset}")
        logger.info(f"  Data: {json.dumps(event, indent=2)}")

        # Update metrics
        messages_received.labels(
            topic=message.topic,
            consumer_group=CONSUMER_GROUP
        ).inc()

    def main():
        # Start Prometheus metrics server
        port = int(os.getenv('METRICS_PORT', '8001'))
        start_http_server(port)
        logger.info(f"Prometheus metrics available at :{port}/metrics")

        # Create consumer
        consumer = create_consumer()
        logger.info(f"Consumer group '{CONSUMER_GROUP}' subscribed to topics: {TOPICS}")
        logger.info(f"Auto offset reset: {AUTO_OFFSET_RESET}")

        # Consume messages
        for message in consumer:
            try:
                process_message(message)
            except Exception as e:
                logger.error(f"Error processing message: {e}")

    if __name__ == '__main__':
        main()
